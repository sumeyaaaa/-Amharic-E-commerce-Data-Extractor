# -Amharic-E-commerce-Data-Extractor
Amharic E-Commerce Entity Extraction is a machine learning pipeline that scrapes Amharic Telegram e-commerce posts and fine-tunes a multilingual transformer model to extract key business entities like Product, Price, and Location,  helping EthioMart become the central hub for Telegram-based digital commerce in Ethiopia.

---
## ğŸ“ Directory Structure of AMHARIC-E-COMMERCE-DATA-EXTRACTOR/

â”œâ”€â”€ .github/
â”œâ”€â”€ .venv/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ processed/
â”‚ â”‚ â”œâ”€â”€ conull.csv # Final labeled data in CoNLL table format
â”‚ â”‚ â”œâ”€â”€ telegram_scraped_data_cleaned.csv# Cleaned Telegram messages
â”‚ â”‚ â””â”€â”€ top_30_messages_per_channel.csv # Selected top messages per channel for annotation
â”‚ â”œâ”€â”€ raw/
â”‚ â”‚ â”œâ”€â”€ images/ # Folder for downloaded product images
â”‚ â”‚ â””â”€â”€ telegram_scraped_data.csv # Raw scraped messages
â”‚
â”œâ”€â”€ models/ # Folder for storing fine-tuned NER models
â”‚
â”œâ”€â”€ notebook/
â”‚ â”œâ”€â”€ task-1/
â”‚ â”‚ â”œâ”€â”€ normalization_and_tokenization.ipynb # Preprocessing pipeline notebook
â”‚ â”‚ â”œâ”€â”€ scrapper_session.session # Telegram session file
â”‚ â”‚ â””â”€â”€ scrapping.ipynb # Data scraping script using Telethon
â”‚ â”œâ”€â”€ task-2/
â”‚ â”‚ â”œâ”€â”€ coNull.ipynb # CoNLL labeling and coverage analysis
â”‚ â”‚ â””â”€â”€ conll_ready_tokenized.txt # Token-per-line file for manual annotation
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ config.py # Channel list, output paths, phone number
â”‚ â”œâ”€â”€ pre_processing.py # Amharic text normalization/cleaning
â”‚ â”œâ”€â”€ scrapper.py # Telegram client & scraping logic
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md # Project documentation (you are here)

yaml
Copy
Edit

## ğŸ§  Key Tasks

### Task 1: Data Ingestion and Preprocessing
- Scrapes messages from Ethiopian Telegram e-commerce channels.
- Cleans and normalizes Amharic text.
- Outputs: `telegram_scraped_data_cleaned.csv`

### Task 2: NER Data Preparation
- Selects top 30 longest messages per channel for rich labeling.
- Tokenizes messages and exports to CoNLL-ready `.txt` format.
- Manual annotation stored in `conull.csv`.
- Outputs: 
  - `top_30_messages_per_channel.csv`  
  - `conll_ready_tokenized.txt`  
  - `conull.csv`

### Task 3â€“5 (Planned):
- Fine-tune multilingual NER models (XLM-Roberta, BERT, etc.)
- Evaluate model with metrics: F1-score, Precision, Recall
- Interpret predictions using SHAP or LIME
- Save final model under `models/`

---

## ğŸ“Œ Labels Used for Annotation

- `O` â€” Outside any entity
- `B-PRODUCT`, `I-PRODUCT`
- `B-AUDIENCE`
- `B-BRAND`, `I-BRAND`
- `B-COMPONENT`, `I-COMPONENT`
- `B-TASK`, `I-TASK`
- `B-CONTACT_INFO`
- `B-PRICE`, `I-PRICE`
- `B-LOC`, `I-LOC`
- `B-DATE`, `I-DATE`
- `B-FEATURE`, `I-FEATURE`
- `B-ATTRIBUTE`, `I-ATTRIBUTE`

---

## ğŸ“Š Labeling Stats
After cleaning:
- **Total tokens** = _N_
- **Labeled tokens (not 'O')** = _M_
- **Coverage** = _(M / N) * 100%_

(To be updated after each labeling batch)

---

## ğŸš€ Future Plans
- Complete model training and evaluation.
- Integrate prediction pipeline into EthioMartâ€™s backend.
- Build a scoring engine to rank vendors based on posting frequency, product diversity, and customer engagement.

---

## ğŸ§ª Setup Instructions

### Requirements
- Python 3.8+
- pandas, regex, telethon
- Jupyter Notebook

### Run scraping:
```bash
jupyter notebook notebook/task-1/scrapping.ipynb
