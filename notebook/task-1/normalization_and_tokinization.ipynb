{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "738acc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Step 1: Set project root path (go up one level from notebooks/)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(),\"..\",\"..\"))\n",
    "\n",
    "# Step 2: Add project root to system path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb37ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src folder to path\n",
    "sys.path.append(os.path.abspath('../../src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4646fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\ABC\\Desktop\\10Acadamy\\week 4\\-Amharic-E-commerce-Data-Extractor\\data\\raw\\telegram_scraped_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b58fb43",
   "metadata": {},
   "source": [
    "# Amharic Text Preprocessing Script\n",
    "\n",
    "## Purpose\n",
    "This script is used to clean and normalize Amharic text data by:\n",
    "- Removing unwanted symbols and emojis\n",
    "- Normalizing whitespace\n",
    "- Retaining essential Amharic characters and punctuation\n",
    "- Lowercasing English text (optional)\n",
    "\n",
    "This process is essential for preparing data for downstream NLP tasks such as Named Entity Recognition (NER).\n",
    "\n",
    "---\n",
    "\n",
    "## Code Usage\n",
    "\n",
    "```python\n",
    "from pre_processing import preprocess_amharic_text\n",
    "df['text_cleaned'] = df['text'].apply(preprocess_amharic_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e62e4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_processing import preprocess_amharic_text\n",
    "df['text_cleaned'] = df['text'].apply(preprocess_amharic_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77a70dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length'] = df['text_cleaned'].str.len()\n",
    "df = df.sort_values(by='text_length', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43d101fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\ABC\\Desktop\\10Acadamy\\week 4\\-Amharic-E-commerce-Data-Extractor\\data\\processed\\telegram_scraped_data_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0c456",
   "metadata": {},
   "source": [
    "# Preparing for coNLL Labeling\n",
    "\n",
    "## Objective\n",
    "The objective of this task is to create high-quality training data for Named Entity Recognition (NER) by manually labeling Amharic Telegram messages. These labels will help train a model that can extract key business entities relevant to the e-commerce domain in Ethiopia.\n",
    "\n",
    "## Selection Criteria\n",
    "To ensure a rich context for labeling:\n",
    "- Messages are sorted by text length.\n",
    "- The **top 30 longest messages** are selected **per Telegram channel**.\n",
    "- Messages with empty or zero-length text are excluded.\n",
    "- Only the text column is retained for annotation purposes.\n",
    "\n",
    "## Export Format\n",
    "Selected messages are saved as a `.csv` file and prepared for manual annotation. The format used for annotation is **CoNLL**, which is widely adopted for NER tasks.\n",
    " ready for CoNLL Format Guidelines\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d790d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ABC\\\\Desktop\\\\10Acadamy\\\\week 4\\\\-Amharic-E-commerce-Data-Extractor\\\\data\\\\processed\\\\top_30_messages_per_channel.csv'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by channel and select top 10 longest messages\n",
    "top_samples = df.sort_values(by='text_length', ascending=False).groupby('channel_username').head(30)\n",
    "# Drop rows where text length == 0\n",
    "df = df[df['text_length'] > 0]\n",
    "\n",
    "# Keep only text column\n",
    "df = df[['text']]\n",
    "# Save selected samples to CSV for review or labeling\n",
    "output_path = r\"C:\\Users\\ABC\\Desktop\\10Acadamy\\week 4\\-Amharic-E-commerce-Data-Extractor\\data\\processed\\top_30_messages_per_channel.csv\"\n",
    "top_samples[['channel_username', 'text', 'text_length']].to_csv(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
